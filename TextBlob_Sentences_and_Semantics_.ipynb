{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextBlob: Sentences and Semantics .ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89Bsf27rzqFY"
      },
      "source": [
        "# **Assignment 4**: TextBlob: Sentences and Semantics\n",
        "Emily Daskas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxNxF--0znN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3ed1567-5613-4e91-a83e-e68f1fe5a48f"
      },
      "source": [
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import spacy \n",
        "nlp = spacy.load('en')\n",
        "from textblob import Word\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from datascience import *\n",
        "from pathlib import Path\n",
        "!pip install textatistic\n",
        "from textatistic import Textatistic\n",
        "\n",
        "#ReadtheRomeo & Juliettext.\n",
        "#Retrieveonlythefirst1,000charactersfromthetext\n",
        "textTotal = open('/RomeoJuliet.txt').read() \n",
        "blobTotal = TextBlob(textTotal)\n",
        "######################################\n",
        "numChars= 1000\n",
        "text1000 = textTotal[0:numChars+1]\n",
        "blob1000 =TextBlob(text1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Requirement already satisfied: textatistic in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: pyhyphen>=2.0.5 in /usr/local/lib/python3.7/dist-packages (from textatistic) (4.0.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pyhyphen>=2.0.5->textatistic) (1.4.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pyhyphen>=2.0.5->textatistic) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pyhyphen>=2.0.5->textatistic) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pyhyphen>=2.0.5->textatistic) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pyhyphen>=2.0.5->textatistic) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pyhyphen>=2.0.5->textatistic) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esiIoNtH1nS6"
      },
      "source": [
        "#### Analysis of Sentences "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0tANBTb2G9Y",
        "outputId": "210f3dce-67fe-41fb-b802-1af6f4f9879b"
      },
      "source": [
        "print(\"1) Total number of sentences: \", len(blobTotal.sentences)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1) Total number of sentences:  3169\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4S0JWI3T2OOp",
        "outputId": "3bdf9baa-581c-4db2-ea60-e1729e18923a"
      },
      "source": [
        "stringQueen = \"Queen\"\n",
        "count = 0\n",
        "for sentence in blobTotal.sentences:\n",
        "  if stringQueen in sentence:\n",
        "    count += 1\n",
        "print('2) Total number of sentences that contain the word \"Queen\":', count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2) Total number of sentences that contain the word \"Queen\": 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "EicxEX8r2tGP",
        "outputId": "9c19906f-1037-4c2c-a229-3f891e6ef78a"
      },
      "source": [
        "sentence_lens = []\n",
        "\n",
        "for sentence in blobTotal.sentences:\n",
        "  sentence_lens.append(len(sentence))\n",
        "\n",
        "t1 = Table().with_column(\"Sentence\", blobTotal.sentences, \"Length\", sentence_lens)\n",
        "sorted_longest_length_table = t1.sort(\"Length\", descending = True)\n",
        "\n",
        "print(\"3) 10 longest sentences in Romeo and Juliet:\")\n",
        "sorted_longest_length_table.show(10)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3) 10 longest sentences in Romeo and Juliet:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/datascience/tables.py:483: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  values = np.array(tuple(values))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>Sentence</th> <th>Length</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>And in this state she gallops night by night\n",
              "Through lov ...</td> <td>857   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>INDEMNITY - You agree to indemnify and hold the Foundati ...</td> <td>683   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>She is the fairies’ midwife, and she comes\n",
              "In shape no b ...</td> <td>649   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>Or, if I live, is it not very like,\n",
              "The horrible conceit ...</td> <td>550   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>The following sentence, with active links to, or other\n",
              "i ...</td> <td>533   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>Day, night, hour, ride, time, work, play,\n",
              "Alone, in comp ...</td> <td>508   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>I do remember an apothecary,—\n",
              "And hereabouts he dwells,— ...</td> <td>493   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>START: FULL LICENSE\n",
              "\n",
              "THE FULL PROJECT GUTENBERG LICENSE\n",
              " ...</td> <td>471   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>However, if you provide access\n",
              "to or distribute copies o ...</td> <td>465   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>[_Exeunt._]\n",
              "\n",
              "\n",
              "End of the Project Gutenberg EBook of Rome ...</td> <td>436   </td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>\n",
              "<p>... (3159 rows omitted)</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "EEiRmKfg43LW",
        "outputId": "3f55b7f8-0630-4efc-c216-30cb92732a97"
      },
      "source": [
        "sorted_shortest_length_table = t1.sort(\"Length\", descending = False)\n",
        "\n",
        "print(\"4) 10 shortest sentences in Romeo and Juliet:\")\n",
        "sorted_shortest_length_table.show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4) 10 shortest sentences in Romeo and Juliet:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>Sentence</th> <th>Length</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>No.     </td> <td>3     </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>Up.     </td> <td>3     </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>No.     </td> <td>3     </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>Go.     </td> <td>3     </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>Ha!     </td> <td>3     </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>No.     </td> <td>3     </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>How?    </td> <td>4     </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>Out.    </td> <td>4     </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>Tut!    </td> <td>4     </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>1.B.    </td> <td>4     </td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>\n",
              "<p>... (3159 rows omitted)</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu0KMmd_83WK"
      },
      "source": [
        "#### Analysis of Sentiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-liuGQko87xA",
        "outputId": "1bfe725d-8071-47f9-fe17-377984a1f5ce"
      },
      "source": [
        "pos_polarity = 0\n",
        "neg_polarity = 0\n",
        "neutral_polarity = 0\n",
        "total_sentences = 0\n",
        "for sentence in blobTotal.sentences:\n",
        "  if sentence.polarity > 0:\n",
        "    pos_polarity += 1\n",
        "  elif sentence.polarity == 0:\n",
        "    neutral_polarity += 1\n",
        "  else:\n",
        "    neg_polarity += 1\n",
        "\n",
        "  total_sentences += 1\n",
        "\n",
        "print(\"1)\")\n",
        "print(\"Total number of Positive sentences:\", pos_polarity)\n",
        "print(\"Total number of Negative sentences:\", neg_polarity)\n",
        "print(\"Total number of Neutral sentences:\", neutral_polarity)\n",
        "print(\"Total number of sentences:\", total_sentences, \"is the same as sum of positive, negative, and neutral polarity sentences:\", pos_polarity + neg_polarity + neutral_polarity)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1)\n",
            "Total number of Positive sentences: 666\n",
            "Total number of Negative sentences: 279\n",
            "Total number of Neutral sentences: 2224\n",
            "Total number of sentences: 3169 is the same as sum of positive, negative, and neutral polarity sentences: 3169\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2U7hcmy9_p_",
        "outputId": "ad7879b8-8576-43c9-e90c-12288d6cb9f1"
      },
      "source": [
        "sentence_polarities = []\n",
        "\n",
        "for sentence in blobTotal.sentences:\n",
        "  sentence_polarities.append(sentence.polarity)\n",
        "\n",
        "polarity_table = Table().with_column(\"Sentence\", blobTotal.sentences, \"Polarity\", sentence_polarities)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/datascience/tables.py:483: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  values = np.array(tuple(values))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "y1CEiAaIAtZh",
        "outputId": "ba887bc0-8047-439c-e81f-37e43496f382"
      },
      "source": [
        "sorted_high_polarity_table = polarity_table.sort(\"Polarity\", descending = True)\n",
        "\n",
        "print(\"2) 5 highest polarities\")\n",
        "sorted_high_polarity_table.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2) 5 highest polarities\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>Sentence</th> <th>Polarity</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>Ay, those attires are best.                                 </td> <td>1       </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>Happily met, my lady and my wife!                           </td> <td>1       </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>Beautiful tyrant, fiend angelical,\n",
              "Dove-feather’d raven, ...</td> <td>1       </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>O Tybalt, Tybalt, the best friend I had.                    </td> <td>1       </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>I thought all for the best.                                 </td> <td>1       </td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>\n",
              "<p>... (3164 rows omitted)</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "wmRSVaU___iO",
        "outputId": "1d08f092-c6e2-41d9-8c50-49c0ce3ea3f6"
      },
      "source": [
        "sorted_low_polarity_table = polarity_table.sort(\"Polarity\", descending = False)\n",
        "\n",
        "print(\"3) 5 lowest polarities\")\n",
        "sorted_low_polarity_table.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3) 5 lowest polarities\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>Sentence</th> <th>Polarity</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>Yea, is the worst well?                                     </td> <td>-1      </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>There’s a fearful point!                                    </td> <td>-1      </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>Romeo, come forth; come forth, thou fearful man.            </td> <td>-0.9    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>Despis’d, distressed, hated, martyr’d, kill’d.              </td> <td>-0.9    </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>I could not send it,—here it is again,—\n",
              "Nor get a messen ...</td> <td>-0.9    </td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>\n",
              "<p>... (3164 rows omitted)</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAe-Q4VaBoHq",
        "outputId": "d295dad5-5732-4275-fc14-f0248c372a79"
      },
      "source": [
        "print(\"4)\")\n",
        "\n",
        "for i in range(0,2):\n",
        "  print(\"English:\", sorted_longest_length_table['Sentence'][i])\n",
        "  print(\"Japanese:\", sorted_longest_length_table['Sentence'][i].translate(to='ja'))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4)\n",
            "English: And in this state she gallops night by night\n",
            "Through lovers’ brains, and then they dream of love;\n",
            "O’er courtiers’ knees, that dream on curtsies straight;\n",
            "O’er lawyers’ fingers, who straight dream on fees;\n",
            "O’er ladies’ lips, who straight on kisses dream,\n",
            "Which oft the angry Mab with blisters plagues,\n",
            "Because their breaths with sweetmeats tainted are:\n",
            "Sometime she gallops o’er a courtier’s nose,\n",
            "And then dreams he of smelling out a suit;\n",
            "And sometime comes she with a tithe-pig’s tail,\n",
            "Tickling a parson’s nose as a lies asleep,\n",
            "Then dreams he of another benefice:\n",
            "Sometime she driveth o’er a soldier’s neck,\n",
            "And then dreams he of cutting foreign throats,\n",
            "Of breaches, ambuscados, Spanish blades,\n",
            "Of healths five fathom deep; and then anon\n",
            "Drums in his ear, at which he starts and wakes;\n",
            "And, being thus frighted, swears a prayer or two,\n",
            "And sleeps again.\n",
            "Japanese: そしてこの状態で彼女は夜ごとにギャロップします\n",
            "恋人たちの頭脳を通して、そして彼らは愛を夢見ます。\n",
            "カーテシーをまっすぐに夢見る、廷臣の膝。\n",
            "手数料をまっすぐに夢見ている他の弁護士の指。\n",
            "キスを真っ直ぐに夢見る女性の唇、\n",
            "水ぶくれに悩まされている怒っているマブはどれですか？\n",
            "汚染されたお菓子で彼らの呼吸は次のとおりです。\n",
            "時々、彼女は廷臣の鼻にギャロップします、\n",
            "そして、彼がスーツの匂いを嗅ぐことを夢見ています。\n",
            "そしていつか彼女は什分の一の豚のしっぽを持って来る、\n",
            "眠っているときにパーソンの鼻をくすぐる、\n",
            "それから彼に別の恩恵を夢見ます：\n",
            "時々、彼女は兵士の首を運転します。\n",
            "そして、彼が外国の喉を切ることを夢見ています、\n",
            "違反、ambuscados、スペインの刃、\n",
            "健康の5つの深さ;そしてアノン\n",
            "彼が開始して目覚める耳の太鼓。\n",
            "そして、このように怯えて、1つか2つの祈りを誓います、\n",
            "そしてまた眠ります。\n",
            "\n",
            "English: INDEMNITY - You agree to indemnify and hold the Foundation, the\n",
            "trademark owner, any agent or employee of the Foundation, anyone\n",
            "providing copies of Project Gutenberg-tm electronic works in\n",
            "accordance with this agreement, and any volunteers associated with the\n",
            "production, promotion and distribution of Project Gutenberg-tm\n",
            "electronic works, harmless from all liability, costs and expenses,\n",
            "including legal fees, that arise directly or indirectly from any of\n",
            "the following which you do or cause to occur: (a) distribution of this\n",
            "or any Project Gutenberg-tm work, (b) alteration, modification, or\n",
            "additions or deletions to any Project Gutenberg-tm work, and (c) any\n",
            "Defect you cause.\n",
            "Japanese: 補償-あなたは財団を補償し、保持することに同意します。\n",
            "商標権者、財団の代理人または従業員、誰でも\n",
            "ProjectGutenberg-tm電子作品のコピーを\n",
            "この合意に従い、およびに関連するすべてのボランティア\n",
            "Project Gutenberg-tmの制作、プロモーション、配布\n",
            "電子作品、すべての責任、費用、費用から無害、\n",
            "いずれかから直接的または間接的に発生する法定費用を含む\n",
            "あなたが行う、または発生させる次のこと：（a）これの配布\n",
            "またはProjectGutenberg-tmの作業、（b）変更、修正、または\n",
            "Project Gutenberg-tmの作品への追加または削除、および（c）\n",
            "あなたが引き起こす欠陥。\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6r5cSltHctb"
      },
      "source": [
        "ngrams_polarities = []\n",
        "all_ngrams = blobTotal.ngrams(n = 5)\n",
        "for ngram in all_ngrams:\n",
        "  ngram_str = str(ngram)\n",
        "  ngram_blob = TextBlob(ngram_str)\n",
        "  ngrams_polarities.append(ngram_blob.polarity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "dMmqCe3tSWqo",
        "outputId": "b8b1d714-b5b6-4fb2-aef2-40d14748bfc7"
      },
      "source": [
        "print(\"5) Top 10 nGrams (n=5) with highest polarity\")\n",
        "ngram_polarity_table = Table().with_column(\"nGrams\", all_ngrams, \"Polarity\", ngrams_polarities)\n",
        "sorted_ngram_polarity_table = ngram_polarity_table.sort(\"Polarity\", descending = True)\n",
        "sorted_ngram_polarity_table.show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5) Top 10 nGrams (n=5) with highest polarity\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>nGrams</th> <th>Polarity</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>['LAWRENCE' 'I' 'am' 'the' 'greatest']</td> <td>1       </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>['best' 'array' 'bear' 'her' 'to']    </td> <td>1       </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>['her' 'best' 'array' 'bear' 'her']   </td> <td>1       </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>['in' 'her' 'best' 'array' 'bear']    </td> <td>1       </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>['And' 'in' 'her' 'best' 'array']     </td> <td>1       </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>['is' 'And' 'in' 'her' 'best']        </td> <td>1       </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>['But' 'she' '’' 's' 'best']          </td> <td>1       </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>['those' 'attires' 'are' 'best' 'But']</td> <td>1       </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>['Ay' 'those' 'attires' 'are' 'best'] </td> <td>1       </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>['best' 'robes' 'uncover' '’' 'd']    </td> <td>1       </td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>\n",
              "<p>... (30782 rows omitted)</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irX3jFqYHdTu"
      },
      "source": [
        "### Stemming, Lemmatization, Synonyms, Antonyms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4KvSVVAIC9O"
      },
      "source": [
        "anticipation = Word('anticipation')\n",
        "angry = Word('angry')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pc-K4QjIRhU",
        "outputId": "cfd02ad2-bd2b-4cd0-84f8-4a28642bc506"
      },
      "source": [
        "print(\"1)\", anticipation.stem())\n",
        "\n",
        "print(\"2)\", anticipation.lemmatize())\n",
        "\n",
        "synonyms = set()\n",
        "for synset in angry.synsets:\n",
        "  for lemma in synset.lemmas():\n",
        "    synonyms.add(lemma.name())\n",
        "print(\"3)\",synonyms)\n",
        "\n",
        "lemmas = angry.synsets[0].lemmas()\n",
        "print(\"4)\", lemmas[0].antonyms())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1) anticip\n",
            "2) anticipation\n",
            "3) {'angry', 'raging', 'tempestuous', 'wild', 'furious'}\n",
            "4) [Lemma('unangry.a.01.unangry')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpBjCBlGJxiY",
        "outputId": "eebc4f68-fa15-4f5d-d35c-64a4ba5ae670"
      },
      "source": [
        "romeo = nlp(Path('/RomeoJuliet.txt').read_text())\n",
        "hamlet = nlp(Path('/Hamlet.txt').read_text()) \n",
        "les_mis = nlp(Path('/Les Miserable.txt').read_text()) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9847134017715589"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4upO7-mUT8Dw",
        "outputId": "0224320d-d60f-4a0f-9c1b-2be29bad43db"
      },
      "source": [
        "print(\"6)\")\n",
        "print(\"Romeo and Juliet vs Hamlet:\", romeo.similarity(hamlet))\n",
        "print(\"Romeo and Juliet vs Les Mis:\", romeo.similarity(les_mis))\n",
        "print(\"Hamlet vs Les Mis:\", hamlet.similarity(les_mis))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6)\n",
            "Romeo and Juliet vs Hamlet: 0.9847134017715589\n",
            "Romeo and Juliet vs Les Mis: 0.9114393770895828\n",
            "Hamlet vs Les Mis: 0.937290224741481\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEUYwKMBUbaU"
      },
      "source": [
        "The two shakespear texts were similar with 98.\n",
        "When WS was compared against VH, the scores were lower with 91 and 93."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jebTl6Y1MRZp"
      },
      "source": [
        "### Readibility Assessment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K6p0xZvMhnt"
      },
      "source": [
        "text_total_romeo = open('/RomeoJuliet.txt').read() \n",
        "text_total_hamlet = open('/Hamlet.txt').read() \n",
        "text_total_les_mis = open('/Les Miserable.txt').read() \n",
        "all_texts = [text_total_romeo, text_total_hamlet, text_total_les_mis]\n",
        "text_names = [\"Romeo and Juliet\", \"Hamlet\", \"Les Mis\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiWteALONsXd",
        "outputId": "42621111-2ead-4415-be75-027a9f5bb0c3"
      },
      "source": [
        "idx = 0\n",
        "for text in all_texts:\n",
        "  print(\"Readability of \", text_names[idx])\n",
        "  print(Textatistic(text).dict())\n",
        "  idx+=1 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Readability of  Romeo and Juliet\n",
            "{'char_count': 131781, 'word_count': 29318, 'sent_count': 3403, 'sybl_count': 35399, 'notdalechall_count': 6922, 'polysyblword_count': 1042, 'flesch_score': 95.94310121673479, 'fleschkincaid_score': 2.0174821979546937, 'gunningfog_score': 4.867787989863079, 'smog_score': 6.29026977403745, 'dalechall_score': 7.791850713812807}\n",
            "Readability of  Hamlet\n",
            "{'char_count': 160543, 'word_count': 35310, 'sent_count': 4098, 'sybl_count': 43997, 'notdalechall_count': 8021, 'polysyblword_count': 1459, 'flesch_score': 92.67598280115092, 'fleschkincaid_score': 2.473442610171446, 'gunningfog_score': 5.099348875241379, 'smog_score': 6.537782921689099, 'dalechall_score': 7.6507209880858635}\n",
            "Readability of  Les Mis\n",
            "{'char_count': 562037, 'word_count': 122869, 'sent_count': 7965, 'sybl_count': 157191, 'notdalechall_count': 25994, 'polysyblword_count': 6652, 'flesch_score': 82.94548596276992, 'fleschkincaid_score': 5.522374808844765, 'gunningfog_score': 8.33600414022729, 'smog_score': 8.349793125977707, 'dalechall_score': 7.742146380254521}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccRo0DBdNrog"
      },
      "source": [
        "In terms of years of education required to understand the text, Les Mis would need to most years: 8.35, followed by Hamlet and then Romeo and Juliet, both requiring around 6 years.\n",
        "\n",
        "Again, according to flesh reading ease score, Les Mis is the least readable, as it is the only one < 90, with a score of 82.9, indicatingit is not readable by 5th graders. \n",
        "\n",
        "In most all categories, Les Mis is the least readable."
      ]
    }
  ]
}